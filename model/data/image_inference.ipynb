{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11b5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import PIL.Image as Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.optim import lr_scheduler\n",
    "from glob import glob\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cwd = Path().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f0be8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_classes):\n",
    "  model = models.resnet34(pretrained=False)\n",
    "\n",
    "  n_features = model.fc.in_features\n",
    "  model.fc = nn.Linear(n_features, n_classes)\n",
    "\n",
    "  return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72aac7c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([31, 512]) from checkpoint, the shape in current model is torch.Size([12, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([31]) from checkpoint, the shape in current model is torch.Size([12]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3da72aba413f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mcolor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcolor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model_state_color.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mfurniture_type_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vision_data/furniture/type_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1224\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([31, 512]) from checkpoint, the shape in current model is torch.Size([12, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([31]) from checkpoint, the shape in current model is torch.Size([12])."
     ]
    }
   ],
   "source": [
    "fashion_type_classes = 18\n",
    "fashion_pattern_classes = 36\n",
    "furniture_type_classes = 10\n",
    "furniture_materials_classes = 7\n",
    "color_classes=12\n",
    "\n",
    "fashion_type_model = None\n",
    "fashion_pattern_model = None\n",
    "furniture_type_model = None\n",
    "furniture_materials_model = None\n",
    "color_model = None\n",
    "###\n",
    "fashion_type_model = create_model(fashion_type_classes)\n",
    "fashion_type_model.load_state_dict(torch.load('best_model_state_fashion_type.bin'))\n",
    "\n",
    "fashion_pattern_model = create_model(fashion_pattern_classes)\n",
    "fashion_pattern_model.load_state_dict(torch.load('best_model_state_fashion_pattern.bin'))\n",
    "\n",
    "furniture_type_model = create_model(furniture_type_classes)\n",
    "furniture_type_model.load_state_dict(torch.load('best_model_state_furniture_type.bin'))\n",
    "\n",
    "furniture_materials_model = create_model(furniture_materials_classes)\n",
    "furniture_materials_model.load_state_dict(torch.load('best_model_state_furniture_materials.bin'))\n",
    "\n",
    "color_model = create_model(color_classes)\n",
    "color_model.load_state_dict(torch.load('best_model_state_color.bin'))\n",
    "\n",
    "furniture_type_folder = \"vision_data/furniture/type_data\"\n",
    "fashion_type_folder = \"vision_data/fashion/type_data\"\n",
    "furniture_materials_folder = \"vision_data/furniture/materials_data\"\n",
    "fashion_pattern_folder = \"vision_data/fashion/pattern_data\"\n",
    "color_folder = \"vision_data/color/color_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca41b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "furniture_type_labels = ['AreaRug', 'Bed', 'Chair', 'CoffeeTable', 'CouchChair', 'EndTable', 'Lamp', 'Shelves', 'Sofa', 'Table']\n",
    "furniture_materials_labels = ['leather', 'marble', 'memory foam', 'metal', 'natural fibers', 'wood', 'wool']\n",
    "fashion_type_labels = ['blouse', 'coat', 'dress', 'hat', 'hoodie', 'jacket', 'jeans', 'joggers', 'shirt', 'shirt, vest',\n",
    " 'shoes', 'skirt', 'suit', 'sweater', 'tank top', 'trousers', 'tshirt', 'vest']\n",
    "fashion_pattern_labels = ['camouflage', 'canvas', 'cargo', 'checkered', 'checkered, plain', 'denim', 'design', 'diamonds', 'dotted',\n",
    " 'floral', 'heavy stripes', 'heavy vertical stripes', 'holiday', 'horizontal stripes', 'knit', 'leafy design', 'leapard print',\n",
    " 'leather', 'light spots', 'light stripes', 'light vertical stripes', 'multicolored', 'plaid', 'plain', 'plain with stripes on side',\n",
    " 'radiant', 'spots', 'star design', 'streaks', 'stripes', 'text', 'twin colors', 'velvet', 'vertical design', 'vertical stripes',\n",
    " 'vertical striples']\n",
    "color_labels = ['beige', 'black', 'blue', 'brown', 'dark blue', 'dark brown', 'dark green', 'dark grey', 'dark pink', 'dark red',\n",
    "                'dark violet', 'dark yellow', 'dirty green', 'dirty grey', 'golden', 'green', 'grey', 'light blue', 'light grey',\n",
    "                'light orange', 'light pink', 'light red', 'maroon', 'olive', 'orange', 'pink', 'purple', 'red', 'violet', \n",
    "                'white', 'yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b958b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_original_folder = \"vision_data/color/color_original_data\"\n",
    "# color_type_set = sorted(listdir(color_original_folder))\n",
    "# print(color_type_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d84549",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_models={\n",
    "    \"furniture_type\": {\n",
    "        \"model\" : furniture_type_model,\n",
    "        \"folder\" : furniture_type_folder,\n",
    "        \"classes\" : furniture_type_labels\n",
    "    },\n",
    "    \"furniture_materials\": {\n",
    "        \"model\" : furniture_materials_model,\n",
    "        \"folder\" : furniture_materials_folder,\n",
    "        \"classes\" : furniture_materials_labels\n",
    "    },\n",
    "    \"fashion_type\": {\n",
    "        \"model\" : fashion_type_model,\n",
    "        \"folder\" : fashion_type_folder,\n",
    "        \"classes\" : fashion_type_labels\n",
    "    },\n",
    "    \"fashion_pattern\": {\n",
    "        \"model\" : fashion_pattern_model,\n",
    "        \"folder\" : fashion_pattern_folder,\n",
    "        \"classes\" : fashion_pattern_labels\n",
    "    },\n",
    "    \"color\": {\n",
    "        \"model\" : color_model,\n",
    "        \"folder\" : color_folder,\n",
    "        \"classes\" : color_labels\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "  inp = inp.numpy().transpose((1, 2, 0))\n",
    "  mean = np.array([mean_nums])\n",
    "  std = np.array([std_nums])\n",
    "  inp = std * inp + mean\n",
    "  inp = np.clip(inp, 0, 1)\n",
    "  plt.figure(figsize = (20,2))\n",
    "  plt.imshow(inp)\n",
    "  if title is not None:\n",
    "    plt.title(title)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, class_names, data_loader, n_images=6):\n",
    "  model = model.eval()\n",
    "  images_handeled = 0\n",
    "  plt.figure()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "\n",
    "      for j in range(inputs.shape[0]):\n",
    "        images_handeled += 1\n",
    "        ax = plt.subplot(n_images,1, images_handeled)\n",
    "        ax.set_title(f'predicted: {class_names[preds[j]]} true: {class_names[labels[j]]}')\n",
    "        imshow(inputs.cpu().data[j])\n",
    "        ax.axis('off')\n",
    "\n",
    "        if images_handeled == n_images:\n",
    "          return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  predictions = []\n",
    "  real_values = []\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in data_loader:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      predictions.extend(preds)\n",
    "      real_values.extend(labels)\n",
    "  predictions = torch.as_tensor(predictions).cpu()\n",
    "  real_values = torch.as_tensor(real_values).cpu()\n",
    "  return predictions, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc11b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix, class_names):\n",
    "\n",
    "  cm = confusion_matrix.copy()\n",
    "\n",
    "  cell_counts = cm.flatten()\n",
    "\n",
    "  cm_row_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "  row_percentages = [\"{0:.2f}\".format(value) for value in cm_row_norm.flatten()]\n",
    "\n",
    "  cell_labels = [f\"{cnt}\\n{per}\" for cnt, per in zip(cell_counts, row_percentages)]\n",
    "  cell_labels = np.asarray(cell_labels).reshape(cm.shape[0], cm.shape[1])\n",
    "\n",
    "  df_cm = pd.DataFrame(cm_row_norm, index=class_names, columns=class_names)\n",
    "\n",
    "  hmap = sns.heatmap(df_cm, annot=cell_labels, fmt=\"\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True Sign')\n",
    "  plt.xlabel('Predicted Sign');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nums = [0.485, 0.456, 0.406]\n",
    "std_nums = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = T.Compose([\n",
    "  T.Resize(size=[256,256]),\n",
    "  T.CenterCrop(size=256),\n",
    "  T.ToTensor(),\n",
    "  T.Normalize(mean_nums, std_nums)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52df245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(data_type):\n",
    "    image_dataset = ImageFolder(f'{image_models[data_type][\"folder\"]}/val/',transform) \n",
    "    data_loader = DataLoader(image_dataset, batch_size=4, shuffle=True, num_workers=4) \n",
    "    class_names = image_models[data_type][\"classes\"]\n",
    "    show_predictions(image_models[data_type][\"model\"], class_names, data_loader,n_images=6) ########################## 3\n",
    "#     y_pred, y_test = get_predictions(image_models[data_type][\"model\"], data_loader)\n",
    "#     print(len(y_pred))\n",
    "#     print(len(class_names))\n",
    "#     print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "#     cm = confusion_matrix(y_test, y_pred,labels = class_names)\n",
    "#     show_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77491d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_predictions(\"fashion_type\") # \"furniture_type\", \"furniture_materials\", \"fashion_type\", \"fashion_pattern\", \"color\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75fa3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inputs, classes = next(iter(data_loader))\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05926a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, image_path):\n",
    "  img = Image.open(image_path)\n",
    "  img = img.convert('RGB')\n",
    "  img = transform(img).unsqueeze(0)\n",
    "\n",
    "  print(img)\n",
    "\n",
    "  pred = model(img.to(device))\n",
    "  pred = F.softmax(pred, dim=1)\n",
    "  _, pred_label = torch.max(pred, 1)\n",
    "\n",
    "  return pred.detach().cpu().numpy().flatten(), pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3130f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, pred_label = predict_proba(image_models[\"fashion_type\"][\"model\"], 'samples/dress.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7239d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_models[\"fashion_type\"][\"classes\"][pred_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6113e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_confidence(prediction, class_names):\n",
    "  pred_df = pd.DataFrame({\n",
    "    'class_names': class_names,\n",
    "    'values': prediction\n",
    "  })\n",
    "  sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
    "  plt.xlim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52deef47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_prediction_confidence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-63d0706b2332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_prediction_confidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fashion_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"classes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_prediction_confidence' is not defined"
     ]
    }
   ],
   "source": [
    "show_prediction_confidence(pred, image_models[\"fashion_type\"][\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b29533bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0630126e-03 4.3446526e-02 1.6059436e-02 3.4127440e-02 6.0117621e-02\n",
      " 2.0684065e-02 5.9490796e-04 6.0783437e-04 1.8854591e-01 5.8841822e-03\n",
      " 1.3563641e-05 1.1707161e-05 1.1822331e-03 8.2772769e-02 5.1073563e-05\n",
      " 1.8214837e-03 5.3529340e-01 1.7228044e-03]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7bc2e358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.0630126e-03 4.3446526e-02 1.6059436e-02 3.4127440e-02 6.0117621e-02\n",
      " 2.0684065e-02 5.9490796e-04 6.0783437e-04 1.8854591e-01 5.8841822e-03\n",
      " 1.3563641e-05 1.1707161e-05 1.1822331e-03 8.2772769e-02 5.1073563e-05\n",
      " 1.8214837e-03 5.3529340e-01 1.7228044e-03]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304277c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dstc_cyj",
   "language": "python",
   "name": "dstc_cyj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
