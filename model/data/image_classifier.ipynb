{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4db679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/nlplab9/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e2ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename,mode=\"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data,filename):\n",
    "    with open(filename, mode=\"w\") as f:\n",
    "        json.dump(data,f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5488d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26194da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove data\n",
    "# !rm -rf vision_data/color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58cda977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea35156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nlplab9/Desktop/youngjae/youngjae/data/*\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "cwd = Path().resolve()\n",
    "query =cwd / \"*\"\n",
    "print(query)\n",
    "# glob(str(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02f2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'assetType': 'blouse_hanging',\n",
    "#  'customerReview': 3.9,\n",
    "#  'availableSizes': ['XS', 'S', 'XL'],\n",
    "#  'color': 'red, white, yellow',\n",
    "#  'pattern': 'plain',\n",
    "#  'brand': 'The Vegan Baker',\n",
    "#  'sleeveLength': 'long',\n",
    "#  'type': 'blouse',\n",
    "#  'price': 39.99,\n",
    "#  'size': 'XS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "895dc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_folder = \"simmc2_scene_jsons_dstc10_public\"\n",
    "scenes = sorted(glob(str(scene_folder + \"/*_scene.json\")))\n",
    "bboxes = sorted(glob(str(scene_folder + \"/*_bbox.json\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c38a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scene in scenes:\n",
    "#     scene_data=load_json(join(scene_folder,scene))\n",
    "#     print(scene_data.keys())\n",
    "#     print(scene_data[\"scenes\"][0].keys())\n",
    "#     print(len(scene_data[\"scenes\"][0][\"objects\"]))\n",
    "#     print(scene_data[\"scenes\"][0][\"objects\"][0].keys())\n",
    "#     print(scene_data[\"scenes\"][0][\"objects\"][0])\n",
    "#     print(scene_data[\"scenes\"][0][\"relationships\"].keys())\n",
    "#     print(scene_data[\"scenes\"][0][\"relationships\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db1b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path1=str(cwd)+  \"/simmc2_scene_images_dstc10_public_part2\"\n",
    "image_path2=str(cwd)+ \"/simmc2_scene_images_dstc10_public_part1\"\n",
    "image_files1 = [f for f in listdir(image_path1) if isfile(join(image_path1, f))]\n",
    "image_files2 = [f for f in listdir(image_path2) if isfile(join(image_path2, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc477ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETS = ['train', 'val', 'test']\n",
    "DATASETS = ['train', 'val'] # split only by two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f6cbd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERIC VERSION\n",
    "def generate_dataset(dataset_type,label_type): \n",
    "    # dataset_type : \"fashion\"  or \"furniture\" \n",
    "    # label_type : \"type\", \"pattern\", \"color\", \"material\"\n",
    "\n",
    "    type_set = set()\n",
    "    meta_file = f\"{dataset_type}_prefab_metadata_all.json\"\n",
    "    metadata = load_json(meta_file)    \n",
    "    for obj in metadata.values():\n",
    "        if obj[label_type].find(\",\")==-1:\n",
    "            type_set.add(obj[label_type])\n",
    "\n",
    "    # build directories for data\n",
    "    DATA_DIR = Path(f'vision_data/{dataset_type}/{label_type}_data')\n",
    "    ORG_DATA_DIR = Path(f'vision_data/{dataset_type}/original_{label_type}_data')\n",
    "    for ds in DATASETS:\n",
    "        for cls in type_set:\n",
    "            (DATA_DIR / ds / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for cls in type_set:\n",
    "        (ORG_DATA_DIR / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # GENERIC VERSION OF GENERATING ORIGINAL DATA\n",
    "    index2data={}\n",
    "    index=0\n",
    "    scene_folder = \"simmc2_scene_jsons_dstc10_public\"\n",
    "    scenes = sorted(glob(str(scene_folder + \"/*_scene.json\")))\n",
    "    for scene in scenes:\n",
    "        scene_data=load_json(scene)\n",
    "        scene_objects = scene_data[\"scenes\"][0][\"objects\"] #object lists\n",
    "\n",
    "        scene_image = scene.replace(\"_scene.json\",\"\").split(\"/\")[1]\n",
    "    #     print(image_path1 + \"/\" + scene_image + \".png\")\n",
    "    #     print(scene_image)\n",
    "\n",
    "        if scene_image[0]==\"m\":\n",
    "            scene_image=scene_image[2:]\n",
    "        if scene_image + \".png\" in image_files1:\n",
    "            im = cv2.imread(image_path1 + \"/\" + scene_image + \".png\")\n",
    "        elif scene_image + \".png\" in image_files2:\n",
    "            im = cv2.imread(image_path2 + \"/\" + scene_image + \".png\")\n",
    "        else:\n",
    "            print(\"there is no image available\")\n",
    "            continue\n",
    "\n",
    "        # got image and scene data\n",
    "        for obj in scene_objects:\n",
    "            try:\n",
    "                bbox = obj[\"bbox\"]\n",
    "                x=bbox[0]\n",
    "                y=bbox[1]\n",
    "                h=bbox[2]\n",
    "                w=bbox[3]\n",
    "                if h==0 or w==0:\n",
    "                    raise Exception('bounding box size is zero!')\n",
    "                # error if image has \n",
    "                crop_img = im[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "    #             print(e)\n",
    "    #             print(x,y,h,w)\n",
    "    #             print(im)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                crop_img = im[y:y+h, x:x+w]\n",
    "#                 print(metadata[obj[\"prefab_path\"]][label_type].find(\",\"))\n",
    "#                 input()\n",
    "                if metadata[obj[\"prefab_path\"]][label_type].find(\",\")!=-1:\n",
    "#                     print(metadata[obj[\"prefab_path\"]][label_type])\n",
    "                    continue\n",
    "                obj_type = metadata[obj[\"prefab_path\"]][label_type]\n",
    "                index2data[str(index)]=(str(index),obj_type,crop_img)\n",
    "                index +=1\n",
    "\n",
    "            except Exception as e:\n",
    "    #             print(e)\n",
    "                continue\n",
    "\n",
    "    # save photo in the correct folder\n",
    "    for key,value in index2data.items():\n",
    "        cv2.imwrite(str(ORG_DATA_DIR / value[1] / (key+\".jpeg\")), value[2])\n",
    "    \n",
    "    for i, ty in enumerate(type_set):\n",
    "        image_paths = np.array(glob(f'{ORG_DATA_DIR}/{ty}/*.jpeg'))\n",
    "        class_name = ty\n",
    "        print(f'{class_name}: {len(image_paths)}')\n",
    "        np.random.shuffle(image_paths)\n",
    "\n",
    "        ds_split = np.split(\n",
    "        image_paths, \n",
    "        #     indices_or_sections=[int(.8*len(image_paths)), int(.9*len(image_paths))]\n",
    "        indices_or_sections=[int(.9*len(image_paths))]\n",
    "\n",
    "        )\n",
    "        #########\n",
    "        ds_split = [image_paths,ds_split[1]]\n",
    "        dataset_data = zip(DATASETS, ds_split)\n",
    "\n",
    "        for ds, images in dataset_data:\n",
    "            for img_path in images:\n",
    "              shutil.copy(img_path, f'{DATA_DIR}/{ds}/{class_name}/')    \n",
    "    \n",
    "    return DATA_DIR\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f9232594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "design: 1510\n",
      "heavy vertical stripes: 124\n",
      "radiant: 589\n",
      "plaid: 1094\n",
      "checkered: 302\n",
      "leather: 844\n",
      "knit: 1098\n",
      "light spots: 907\n",
      "plain with stripes on side: 899\n",
      "plain: 35324\n",
      "cargo: 32\n",
      "vertical stripes: 896\n",
      "horizontal stripes: 897\n",
      "twin colors: 4295\n",
      "holiday: 887\n",
      "star design: 0\n",
      "vertical striples: 692\n",
      "velvet: 637\n",
      "spots: 1254\n",
      "canvas: 992\n",
      "streaks: 0\n",
      "multicolored: 593\n",
      "leapard print: 0\n",
      "diamonds: 566\n",
      "vertical design: 371\n",
      "camouflage: 1484\n",
      "leafy design: 1124\n",
      "heavy stripes: 797\n",
      "light stripes: 965\n",
      "denim: 4117\n",
      "stripes: 601\n",
      "dotted: 421\n",
      "light vertical stripes: 681\n",
      "floral: 97\n",
      "text: 703\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR=generate_dataset(\"fashion\",\"pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e4248ae5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = generate_dataset(\"furniture\",\"materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817a270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8759e9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For color training\n",
    "# color_original_folder = \"vision_data/color/color_original_data\"\n",
    "# ORG_DATA_DIR = Path(color_original_folder)\n",
    "# color_type_set = sorted(listdir(color_original_folder))\n",
    "# DATA_DIR=Path(\"vision_data/color/color_data\")\n",
    "# for ds in DATASETS:\n",
    "#     for cls in color_type_set:\n",
    "#         (DATA_DIR / ds / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for i, ty in enumerate(color_type_set):\n",
    "#     image_paths = np.array(glob(f'{ORG_DATA_DIR}/{ty}/*.jpg'))\n",
    "#     class_name = ty\n",
    "#     print(f'{class_name}: {len(image_paths)}')\n",
    "#     np.random.shuffle(image_paths)\n",
    "\n",
    "#     ds_split = np.split(\n",
    "#     image_paths, \n",
    "#     #     indices_or_sections=[int(.8*len(image_paths)), int(.9*len(image_paths))]\n",
    "#     indices_or_sections=[int(.9*len(image_paths))]\n",
    "\n",
    "#     )\n",
    "#     print(ds_split)\n",
    "#     input()\n",
    "    \n",
    "\n",
    "#     dataset_data = zip(DATASETS, ds_split)\n",
    "\n",
    "#     for ds, images in dataset_data:\n",
    "#         for img_path in images:\n",
    "#           shutil.copy(img_path, f'{DATA_DIR}/{ds}/{class_name}/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3014624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nums = [0.485, 0.456, 0.406]\n",
    "std_nums = [0.229, 0.224, 0.225]\n",
    "\n",
    "transforms = {'train': T.Compose([\n",
    "  T.RandomResizedCrop(size=[256,256]),\n",
    "  T.RandomRotation(degrees=15),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor(),\n",
    "  T.Normalize(mean_nums, std_nums)\n",
    "]), 'val': T.Compose([\n",
    "  T.Resize(size=[256,256]),\n",
    "  T.CenterCrop(size=256),\n",
    "  T.ToTensor(),\n",
    "  T.Normalize(mean_nums, std_nums)\n",
    "]), 'test': T.Compose([\n",
    "  T.Resize(size=[256,256]),\n",
    "  T.CenterCrop(size=256),\n",
    "  T.ToTensor(),\n",
    "  T.Normalize(mean_nums, std_nums)\n",
    "]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7ab5a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "furniture_type_folder = Path(\"vision_data/furniture/type_data\")\n",
    "fashion_type_folder = Path(\"vision_data/fashion/type_data\")\n",
    "furniture_materials_folder = Path(\"vision_data/furniture/materials_data\")\n",
    "fashion_pattern_folder = Path(\"vision_data/fashion/pattern_data\")\n",
    "color_folder=Path(\"vision_data/color/color_data\")\n",
    "color_furn_folder=Path(\"vision_data/color_furn/color_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1e5de517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Found no valid file for the classes leapard print, star design, streaks. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-cf38d5c83eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ORG_DATA_DIR = Path(\"vision_data/color/original_color_data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m image_datasets = {\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/{d}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-cf38d5c83eb5>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ORG_DATA_DIR = Path(\"vision_data/color/original_color_data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m image_datasets = {\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATA_DIR}/{d}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dstc_cyj/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes leapard print, star design, streaks. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
     ]
    }
   ],
   "source": [
    "# Set input for model\n",
    "# DATA_DIR=Path(\"vision_data/color/color_data\")\n",
    "# DATA_DIR=color_furn_folder\n",
    "# ORG_DATA_DIR = Path(\"vision_data/color/original_color_data\")\n",
    "image_datasets = {\n",
    "  d: ImageFolder(f'{DATA_DIR}/{d}', transforms[d]) for d in DATASETS\n",
    "}\n",
    "\n",
    "data_loaders = {\n",
    "  d: DataLoader(image_datasets[d], batch_size=4, shuffle=True, num_workers=4) \n",
    "  for d in DATASETS\n",
    "}\n",
    "\n",
    "dataset_sizes = {d: len(image_datasets[d]) for d in DATASETS}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets[\"train\"]\n",
    "print(len(image_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84817a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "  inp = inp.numpy().transpose((1, 2, 0))\n",
    "  mean = np.array([mean_nums])\n",
    "  std = np.array([std_nums])\n",
    "  inp = std * inp + mean\n",
    "  inp = np.clip(inp, 0, 1)\n",
    "  plt.imshow(inp)\n",
    "  if title is not None:\n",
    "    plt.title(title)\n",
    "  plt.axis('off')\n",
    "\n",
    "inputs, classes = next(iter(data_loaders['train']))\n",
    "# print(inputs, classes)\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670fa21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e732a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_classes):\n",
    "  model = models.resnet34(pretrained=True)\n",
    "#   model = models.resnext101_32x8d(pretrained=True)\n",
    "  n_features = model.fc.in_features\n",
    "  model.fc = nn.Linear(n_features, n_classes)\n",
    "\n",
    "  return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d20e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for inputs, labels in data_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == labels)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  scheduler.step()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in data_loader:\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, labels)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == labels)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753da1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loaders, dataset_sizes, device, n_epochs=3):\n",
    "  optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.9)\n",
    "  scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "  loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "  history = defaultdict(list)\n",
    "  best_accuracy = 0\n",
    "\n",
    "  for epoch in tqdm(range(n_epochs),total=n_epochs):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "      model,\n",
    "      data_loaders['train'],    \n",
    "      loss_fn, \n",
    "      optimizer, \n",
    "      device, \n",
    "      scheduler, \n",
    "      dataset_sizes['train']\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "      model,\n",
    "      data_loaders['val'],\n",
    "      loss_fn,\n",
    "      device,\n",
    "      dataset_sizes['val']\n",
    "    )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc.cpu())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc.cpu())\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "      torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "      best_accuracy = val_acc\n",
    "\n",
    "  print(f'Best val accuracy: {best_accuracy}')\n",
    "  \n",
    "  model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "\n",
    "  return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2300536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model\n",
    "base_model = create_model(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99d46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "base_model, history = train_model(base_model, data_loaders, dataset_sizes, device,n_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50b19e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "  ax1.plot(history['train_loss'], label='train loss')\n",
    "  ax1.plot(history['val_loss'], label='validation loss')\n",
    "\n",
    "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax1.set_ylim([-0.05, 1.05])\n",
    "  ax1.legend()\n",
    "  ax1.set_ylabel('Loss')\n",
    "  ax1.set_xlabel('Epoch')\n",
    "\n",
    "  ax2.plot(history['train_acc'], label='train accuracy')\n",
    "  ax2.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "  ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax2.set_ylim([-0.05, 1.05])\n",
    "  ax2.legend()\n",
    "\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_xlabel('Epoch')\n",
    "\n",
    "  fig.suptitle('Training history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e29241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d30d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412485c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed9f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dstc_cyj",
   "language": "python",
   "name": "dstc_cyj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
